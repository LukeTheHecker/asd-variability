{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Intertrial Variability: 2. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(1, '../')\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(font_scale=1.,style='ticks',context='notebook',font= 'georgia')\n",
    "from scipy.stats import ttest_ind\n",
    "import pingouin\n",
    "from scripts.preproc import *\n",
    "from scripts.viz import *\n",
    "from sklearn import linear_model\n",
    "from scipy.signal import detrend\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../processed/data_preprocessed.pkl'\n",
    "\n",
    "with open(fn, 'rb') as f:\n",
    "    [all_data_raw, all_data_epoch] = pkl.load(f)\n",
    "# Load AQ and EQ data\n",
    "fn = 'D:/data/Psychiatrie_Autismus_2012/AQ_EQ.json'\n",
    "\n",
    "AQ, EQ = get_AQ_EQ(fn, all_data_epoch)\n",
    "\n",
    "# Convert Dict to list cause thats how i like it\n",
    "all_data_epoch = [list(all_data_epoch['control'].values()), list(all_data_epoch['asd'].values())]\n",
    "info = all_data_epoch[0][0].info\n",
    "times = all_data_epoch[0][0].times\n",
    "ch_names = all_data_epoch[0][0].ch_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_picks = ['O1', 'Oz', 'O2']\n",
    "ch_indices = [ch_names.index(ch) for ch in ch_picks]\n",
    "conds = [['SF', 'LF'], ['GPL', 'GPS']]\n",
    "cond_labels = ['Frequent checkers',  'Grey blanks']\n",
    "\n",
    "time_range = [-0.2, 0.5]\n",
    "pnt_range = np.arange(*[np.argmin(np.abs(times-t)) for t in time_range])\n",
    "scaler = 1e6\n",
    "\n",
    "itv_conds = [[np.stack(list(map(partial(calc_itv, cond=cond, norm_type=None, relative=False), epochs)), axis=0) \n",
    "            for epochs in all_data_epoch] \n",
    "            for cond in conds]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (15, 5)\n",
    "fig = plt.figure(num='ITV', figsize=figsize)\n",
    "sns.set(font_scale=1.4)\n",
    "for i, (cond, itv) in enumerate(zip(conds, itv_conds)):\n",
    "    itv = [scaler * x[:, :, pnt_range] for x in itv]\n",
    "    N = 121+i\n",
    "    ax = plt.subplot(N)\n",
    "    plot_itv(itv, times[pnt_range], ax, info, cond=cond, title=cond_labels[i], alpha=0.05, ch_picks=ch_picks)\n",
    "    ax.set_ylim((0, plt.ylim()[1]))\n",
    "    if i == 0:\n",
    "        ax.set_xlabel('Time [s]')\n",
    "        ax.set_ylabel('ITV [\\u03BCV]')\n",
    "    \n",
    "ax.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# multipage(r'C:\\Users\\Lukas\\Documents\\VariabilityAnalysis\\figures\\paper\\for_review\\ITV.pdf', figs=[fig], dpi=300, png=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_picks = ['O1', 'Oz', 'O2']\n",
    "ch_indices = [ch_names.index(ch) for ch in ch_picks]\n",
    "conds = [['SF', 'LF'], ['GPL', 'GPS']]\n",
    "cond_labels = ['Frequent checkers',  'Grey blanks']\n",
    "\n",
    "\n",
    "# Subject List\n",
    "subjects = np.concatenate([\n",
    "    ['Control_'+str(i+1) for i in range(len(all_data_epoch[0]))], \n",
    "    ['ASD_'+str(i+1) for i in range(len(all_data_epoch[1]))]\n",
    "    ])\n",
    "groups = np.concatenate([\n",
    "    ['Control' for _ in range(len(all_data_epoch[0]))], \n",
    "    ['ASD' for _ in range(len(all_data_epoch[1]))]\n",
    "    ])\n",
    "\n",
    "df_list = []\n",
    "# loop through Stimuli\n",
    "for i, (itv, cond, cond_label) in enumerate(zip(itv_conds, conds, cond_labels)):\n",
    "    # Prepare Data of the current stimulus    \n",
    "    data = np.concatenate([np.mean(itv[0][:, ch_indices, :], axis=-1), np.mean(itv[1][:, ch_indices, :], axis=-1)], axis=0)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=ch_picks)\n",
    "    df['Subject'] = subjects\n",
    "    df['Group'] = groups\n",
    "    df['Stimulus'] = [cond_label] * data.shape[0]\n",
    "    # Melt individual electrode columns into a single Column\n",
    "    df_itv = df.melt(value_vars=ch_picks, \n",
    "                    id_vars=['Subject', 'Group', 'Stimulus'], \n",
    "                    value_name='ITV', var_name='Electrode')\n",
    "    df_list.append(df_itv)\n",
    "# Concatenate DataFrames of the different stimuli\n",
    "df_itv = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable = 'ITV'\n",
    "factors = ['Group', 'Stimulus']\n",
    "aov = df_itv.anova(dv=dependent_variable, between=factors, detailed=True, effsize='n2')\n",
    "\n",
    "\n",
    "# post-hoc test\n",
    "post_hoc = df_itv.pairwise_tukey(dv=dependent_variable, between='Group')\n",
    "display(aov)\n",
    "print('\\nPost-Hoc:\\n')\n",
    "display(post_hoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_picks = ['O1', 'Oz', 'O2']\n",
    "ch_indices = [ch_names.index(ch) for ch in ch_picks]\n",
    "conds = ['SF', 'LF', 'GPL', 'GPS']\n",
    "cond_labels = ['Small frequent', 'Large frequent', 'Grey screen L', 'Grey screen S']\n",
    "\n",
    "time_range = [-0.2, 0.5]\n",
    "pnt_range = [np.argmin(np.abs(times-time_range[0])), \n",
    "            np.argmin(np.abs(times-time_range[1]))]\n",
    "pnt_range = np.arange(*pnt_range)\n",
    "\n",
    "# Calc ETV\n",
    "etv_conds = [[list(map(partial(calc_etv, cond=cond), epochs)) \n",
    "        for epochs in all_data_epoch] \n",
    "        for cond in conds]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (4, 8.5)\n",
    "scaler = 1e6\n",
    "\n",
    "figsize = (12, 7)\n",
    "fig = plt.figure(num='ETV', figsize=figsize)\n",
    "sns.set(font_scale=1.2)\n",
    "plt.suptitle('Evolving Inter-Trial Variability')\n",
    "for i, (etv, cond) in enumerate(zip(etv_conds, conds)):\n",
    "    params = dict(cond=cond)\n",
    "    \n",
    "    # Cut of trials so all participants have the same amount\n",
    "    min_tr = min([min([sub.shape[0] for sub in eee]) for eee in etv])\n",
    "    etv = [np.stack([sub[:min_tr, ch_indices, :]*scaler for sub in eee], axis=0) for eee in etv]\n",
    "    # Average over time and re-arrange\n",
    "    etv = [np.swapaxes(np.mean(group[..., pnt_range], axis=-1), 1, 2) for group in etv]\n",
    "    # Plot\n",
    "    N = 221+i\n",
    "    ax = plt.subplot(N)\n",
    "    plot_etv(etv, ax, info, cond=cond, title=cond_labels[i])\n",
    "    ax.set_ylim(ylim)\n",
    "    if i == 2:\n",
    "        ax.set_xlabel('Trial No.')\n",
    "        ax.set_ylabel('ETV [\\u03BCV]')\n",
    "    \n",
    "ax.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "plt.tight_layout(pad=2)\n",
    "\n",
    "# multipage(r'C:\\Users\\Lukas\\Documents\\VariabilityAnalysis\\figures\\paper\\for_review\\ETV.pdf', figs=[fig], dpi=300, png=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds = ['SF', 'LF',  'GPL', 'GPS']\n",
    "cond_labels = ['Small frequent', 'Large frequent',  'Grey screen L', 'Grey screen S']\n",
    "experiments = ['LR', 'SR', 'LR', 'SR']\n",
    "etv = etv_conds[0]\n",
    "# Subject List\n",
    "subjects = np.concatenate([\n",
    "    ['Control_'+str(i+1) for i in range(len(all_data_epoch[0]))], \n",
    "    ['ASD_'+str(i+1) for i in range(len(all_data_epoch[1]))]\n",
    "    ])\n",
    "groups = np.concatenate([\n",
    "    ['Control' for _ in range(len(all_data_epoch[0]))], \n",
    "    ['ASD' for _ in range(len(all_data_epoch[1]))]\n",
    "    ])\n",
    "\n",
    "\n",
    "df_list = []\n",
    "# loop through Stimuli\n",
    "for i, (etv, cond, cond_label, experiment) in enumerate(zip(etv_conds, conds, cond_labels, experiments)):\n",
    "    # Cut of trials so all participants have the same amount\n",
    "    min_tr = min([min([sub.shape[0] for sub in eee]) for eee in etv])\n",
    "    etv = [np.stack([sub[:min_tr, ch_indices, :]*1e6 for sub in eee], axis=0) for eee in etv]\n",
    "    # Average over time and re-arrange\n",
    "    etv = [np.swapaxes(np.mean(group[..., pnt_range], axis=-1), 1, 2) for group in etv]\n",
    "    # Prepare Data of the current stimulus    \n",
    "    data = np.concatenate([np.mean(etv[0], axis=-1), np.mean(etv[1], axis=-1)], axis=0)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=ch_picks)\n",
    "    df['Subject'] = subjects\n",
    "    df['Group'] = groups\n",
    "    df['Stimulus'] = [cond_label] * data.shape[0]\n",
    "    df['Condition'] = [experiment] * data.shape[0]\n",
    "    # Melt individual electrode columns into a single Column\n",
    "    df_etv= df.melt(value_vars=ch_picks, \n",
    "                    id_vars=['Subject', 'Group', 'Stimulus', 'Condition'], \n",
    "                    value_name='ETV', var_name='Electrode')\n",
    "    df_list.append(df_etv)\n",
    "# Concatenate DataFrames of the different stimuli\n",
    "df_etv = pd.concat(df_list)\n",
    "df_etv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable = 'ETV'\n",
    "factors = ['Group', 'Stimulus']\n",
    "aov = df_etv.anova(dv=dependent_variable, between=factors, \n",
    "    detailed=True, effsize='n2')\n",
    "\n",
    "# post-hoc test\n",
    "ph_test = df_etv.pairwise_tukey(dv=dependent_variable, between='Group')\n",
    "display(aov)\n",
    "print('\\nPost Hoc:\\n')\n",
    "display(ph_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETV Progression Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slope = pd.DataFrame(columns=['Subject', 'Group', 'Condition', 'Slope', 'ETVVar', 'ETVVar_detrended', 'Residual'])\n",
    "groups = ['Control', 'ASD']\n",
    "cond_labels = ['Small frequent', 'Large frequent', 'Grey screen L', 'Grey screen S']\n",
    "\n",
    "for cond in range(len(etv_conds)):\n",
    "    for group in range(len(etv_conds[cond])):\n",
    "        for sub in range(len(etv_conds[cond][group])):\n",
    "            subject = 'S' + str(sub)\n",
    "            groupname = groups[group]\n",
    "            conditionname = cond_labels[cond]\n",
    "            etv_sample = np.mean(etv_conds[cond][group][sub][:, ch_indices, :], axis=(1, 2))\n",
    "    \t    # Normalize\n",
    "            # etv_sample = (etv_sample - etv_sample.mean()) \n",
    "            etv_sample = (etv_sample / etv_sample.mean()) \n",
    "            \n",
    "            etv_var = np.std(etv_sample)\n",
    "            etv_var_detrended = np.std(detrend(etv_sample))\n",
    "            # Get slope\n",
    "            Y = etv_sample.reshape(-1, 1)\n",
    "            X = np.arange(len(etv_sample)).reshape(-1, 1)\n",
    "               \n",
    "            lr = linear_model.HuberRegressor().fit(X, Y)\n",
    "            line = lr.predict(X)\n",
    "            slope = np.diff(np.squeeze(line))[0]\n",
    "            residual = rms(etv_sample-line)\n",
    "\n",
    "            # First half vs. second half variability\n",
    "            first_half, second_half = np.array_split(etv_sample, 2)\n",
    "            rel_var_red = np.std(detrend(second_half)) / np.std(detrend(first_half))\n",
    "\n",
    "            df_slope = df_slope.append({'Subject': subject, 'Group': groupname, \n",
    "                'Condition': conditionname, \n",
    "                'Slope': slope, 'ETVVar': etv_var, 'ETVVar_detrended': etv_var_detrended, 'Residual': residual,\n",
    "                'RelVarReduction': rel_var_red}, ignore_index=True)\n",
    "\n",
    "\n",
    "df_slope.anova(dv='Residual', between=[\"Group\"], detailed=True, effsize='np2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_slope.columns[3:]\n",
    "for col in cols:\n",
    "    print(col, '\\n')\n",
    "    display(df_slope.anova(dv=col, between=[\"Group\", \"Condition\"], detailed=True, effsize='np2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data Frame with all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch_picks = ch_names\n",
    "ch_picks = ['O1', 'Oz', 'O2']\n",
    "conds = [['SF', 'LF'], ['GPL', 'GPS']]\n",
    "cond_labels = ['Frequent checkers', 'Grey blanks']\n",
    "scaler = 1e6\n",
    "legend = ['Controls', 'ASD']\n",
    "\n",
    "n_tr_conds = [[[calc_n_tr(epoch, conds=cond, ch_picks=ch_picks) for epoch in group] for group in all_data_epoch] for cond in conds]\n",
    "\n",
    "subjects = np.concatenate([['Control_'+str(i+1) for i in range(len(all_data_epoch[0]))], ['ASD_'+str(i+1) for i in range(len(all_data_epoch[1]))]])\n",
    "groups = np.concatenate([['Control' for _ in range(len(all_data_epoch[0]))], ['ASD' for _ in range(len(all_data_epoch[1]))]])\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Subject'] = subjects\n",
    "df['Group'] = groups\n",
    "\n",
    "for i, cond_label in enumerate(cond_labels):\n",
    "    # df[f'Raw {cond_label}'] = np.concatenate(raw_rms[i]) * scaler\n",
    "    # df[f'ERP {cond_label}'] = np.concatenate(erp_rms[i]) * scaler\n",
    "    df[f'Trials {cond_label}'] = np.concatenate(n_tr_conds[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_everything = df[['Subject', 'Group']]\n",
    "# df_everything = df_everything.rename(columns={\"Raw Frequent checkers\": \"RMS Raw\", \"ERP Frequent checkers\": \"RMS ERP\"})\n",
    "# AQ and EQ\n",
    "df_everything['AQ'] = AQ\n",
    "df_everything['EQ'] = EQ\n",
    "\n",
    "# Add ITV\n",
    "df_tmp = df_itv[df_itv['Stimulus']=='Frequent checkers']\n",
    "df_tmp['Sub Idx'] = np.concatenate([np.arange(35)]*3)\n",
    "df_tmp = df_tmp.pivot_table(index=['Sub Idx', 'Subject', 'Group'], columns='Electrode')\n",
    "df_tmp['ROI'] = np.mean(np.stack([df_tmp[('ITV', 'O1')].values, df_tmp[('ITV', 'Oz')].values, df_tmp[('ITV', 'O2')].values], axis=0), axis=0)\n",
    "df_everything['ITV_ROI'] = df_tmp['ROI'].values * 1e6\n",
    "\n",
    "# Add ETV metrics\n",
    "# frequents_bool = (df_slope['Condition'] == 'Small frequent') + (df_slope['Condition'] == 'Large frequent')\n",
    "etv_slope = np.mean(np.stack([df_slope[df_slope['Condition'] == 'Small frequent']['Slope'].values, df_slope[df_slope['Condition'] == 'Large frequent']['Slope'].values], axis=0), axis=0)\n",
    "etv_variability = np.mean(np.stack([df_slope[df_slope['Condition'] == 'Small frequent']['ETVVar'].values, df_slope[df_slope['Condition'] == 'Large frequent']['ETVVar'].values], axis=0), axis=0)\n",
    "\n",
    "etv_variability_detrended = np.mean(np.stack([df_slope[df_slope['Condition'] == 'Small frequent']['ETVVar_detrended'].values, df_slope[df_slope['Condition'] == 'Large frequent']['ETVVar_detrended'].values], axis=0), axis=0)\n",
    "\n",
    "itv_ratio = np.mean(np.stack([df_slope[df_slope['Condition'] == 'Small frequent']['RelVarReduction'].values, df_slope[df_slope['Condition'] == 'Large frequent']['RelVarReduction'].values], axis=0), axis=0)\n",
    "df_everything['ETV-slope'] = etv_slope*1e6\n",
    "df_everything['ETV-variability'] = etv_variability*1e6\n",
    "df_everything['ETV-variability_detrended'] = etv_variability_detrended*1e6\n",
    "df_everything['ITV-ratio'] = itv_ratio\n",
    "\n",
    "\n",
    "df_everything.to_csv(os.path.join('../', 'processed','dataframe_asd_2.csv'))\n",
    "df_everything.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1 Coefficient of Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "ch_picks = None  # None -> Select the channel with highest P1 peak response\n",
    "conds = ['SF', 'LF',]  # Select only trials in which Small Frequent and Large Frequent checkers were shown\n",
    "time_range = [-0.2, 0.5]  # Default time range for this study\n",
    "pnt_range = np.arange(*[np.argmin(np.abs(times-t)) for t in time_range])\n",
    "scaler = 1e6  # Scale from Volts to Microvolts\n",
    "baseline = (-0.1, 0)  # Baseline as described by Milne (2011)\n",
    "\n",
    "# Calc P1 for each participant\n",
    "p100_conds = [np.stack(list(map(partial(p100_milne, cond=conds, baseline=baseline, ch_name=ch_picks, verbose=0), epochs)), axis=0) \n",
    "            for epochs in all_data_epoch] \n",
    "\n",
    "\n",
    "# Convert to Pandas Series for convenience\n",
    "p1_dict = dict(Controls=p100_conds[0], ASD=p100_conds[1])\n",
    "ser = pd.Series(p1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=1)\n",
    "sns.barplot(data=ser, ci=68)\n",
    "plt.xticks(ticks=[0,1], labels=['Controls', 'ASD'])\n",
    "plt.ylabel('Normalized Peak Amplitude Variability')\n",
    "plt.xlabel('Group')\n",
    "plt.title(f'P1 Variability')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t,p = ttest_ind(ser[0], ser[1])\n",
    "d = cohens_d(ser[0], ser[1])\n",
    "print(f'P1 Variability. t: {t:.2f}, p: {p:.4f}, d: {d:.3f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ab6a524f8fdefa2fba1a0ab7407e8dbd386bc97b991bfb50d01a11f6cd0d5f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('eegenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
