{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12f2789",
   "metadata": {},
   "source": [
    "---\n",
    "# Intertrial-Variability: Classification and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(1, '../')\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from hyperopt import hp, Trials, fmin, tpe, STATUS_OK\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from pathlib import Path\n",
    "from scripts.util import hyperopt_train_test, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b6d08",
   "metadata": {},
   "source": [
    "---\n",
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b5cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of parameters to be tested\n",
    "space4svm = {\n",
    "'C': hp.uniform('C', 0.0001, 100),\n",
    "'gamma': hp.uniform('gamma', 0.0001, 100),\n",
    "}\n",
    "\n",
    "# find best parameters\n",
    "# function to hyperoptimise parameters\n",
    "def hyperopt_train_test(params):\n",
    "    model = SVC(kernel = 'linear', **params)\n",
    "    #scaler for data\n",
    "    scaler = StandardScaler()\n",
    "    #make pipeline inorder to have scaling be part of cv\n",
    "    pipeline = make_pipeline(scaler,model)\n",
    "    # set in cross-validation\n",
    "    cv = LeaveOneOut()\n",
    "    result = cross_val_score(pipeline,cv_set,y_cv,cv=cv,scoring='accuracy')\n",
    "    return result.mean()\n",
    "    \n",
    "def f(params):\n",
    "    acc = hyperopt_train_test(params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df8a26",
   "metadata": {},
   "source": [
    "## Read Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"..\", \"processed\", \"dataframe_asd_2.csv\") )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty df to save accuracies in\n",
    "results_eval = pd.DataFrame(columns=['ITV_ROI','ETV-slope','ETV-variability',\n",
    "                                     'ETV-variability_detrended','ITV-ratio','All Metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6f61e",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_crossvals = 20  # how often the hyperparameter optimization + LOO CV is repeated\n",
    "max_evals = 60  # Number of iterations for hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b1585e",
   "metadata": {},
   "source": [
    "## Get accuracies for all metrics separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-panel",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classification labels\n",
    "y = np.array([0 if i == 'Control' else 1 for i in df['Group']])\n",
    "# metrics to get accuracy from\n",
    "metric = ['ITV_ROI','ETV-slope','ETV-variability','ETV-variability_detrended','ITV-ratio']\n",
    "\n",
    "# empty list to evaluation scores in to\n",
    "evals = []\n",
    "\n",
    "# loop through all metrics\n",
    "for m in metric:\n",
    "    # empty list to put specific metric eval scores in to\n",
    "    metric_eval = []\n",
    "    # do loo-cv for each metric 20 times \n",
    "    for i in range(n_crossvals):\n",
    "        # empty list to put evaluation score in to\n",
    "        evaluation_set = []\n",
    "        # convert df to array\n",
    "        X = df[m].to_numpy()\n",
    "        # loop through array of data\n",
    "        for evalu_index, evalu in enumerate(X):\n",
    "            scaler = StandardScaler()\n",
    "            #create a training fold with all but 1 participanrt\n",
    "            cv_set = X[np.where(np.where(X==X)[0]!=evalu_index)]\n",
    "            cv_set = cv_set.reshape(-1, 1)\n",
    "            y_cv = y[np.where(np.where(y==y)[0]!=evalu_index)]\n",
    "            y_true = y[evalu_index]\n",
    "            \n",
    "            # get best parameters\n",
    "            trials = Trials()\n",
    "            best = fmin(f, space4svm, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "\n",
    "            # instantiate classifier\n",
    "            clf = SVC(kernel = 'linear', **best)\n",
    "            # scale training set \n",
    "            scaler.fit_transform(cv_set)\n",
    "            # train classifier\n",
    "            clf.fit(cv_set,y_cv)\n",
    "            # scale evaluation set based on training scaling\n",
    "            evalu = evalu.reshape(1, -1)\n",
    "            scaler.transform(evalu)\n",
    "            # predict\n",
    "            y_pred = clf.predict(evalu)\n",
    "            # if prediction matches true label\n",
    "            if y_pred == y_true:\n",
    "                result = 1\n",
    "            elif y_pred != y_true:\n",
    "                result = 0\n",
    "            # save evaluation set prediction\n",
    "            evaluation_set.append(result)\n",
    "        # get score of all folds\n",
    "        eval_score = sum(evaluation_set) / len(evaluation_set)\n",
    "        metric_eval.append(eval_score)\n",
    "    evals.append(metric_eval)\n",
    "    # save median of the 20 runs of each metric loo-cv\n",
    "    results_eval.loc['LOO Score (Eval)', m] = np.median(metric_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee2c4a",
   "metadata": {},
   "source": [
    "## Get accuracies for all metrics combined & average distances to hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-isolation",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do the same procedure as above but for all metrics instead of just one\n",
    "# also get average distances\n",
    "y = np.array([0 if i == 'Control' else 1 for i in df['Group']])\n",
    "metric = ['ITV_ROI','ETV-slope','ETV-variability','ETV-variability_detrended','ITV-ratio'] #\n",
    "evaluation_set = []\n",
    "results_distances = np.zeros([len(y)])\n",
    "X = df[metric].to_numpy()\n",
    "\n",
    "evals_all = []\n",
    "for i in range(n_crossvals):\n",
    "    for evalu_index, evalu in enumerate(X):\n",
    "        scaler = StandardScaler()\n",
    "        indices_X = np.where(np.where(X[:,0]==X[:,0])[0]!=evalu_index)\n",
    "        indices_y = np.where(np.where(y==y)[0]!=evalu_index)\n",
    "        cv_set = X[indices_X]\n",
    "        y_cv = y[indices_y]\n",
    "        y_true = y[evalu_index]\n",
    "        \n",
    "        # get best parameters\n",
    "        trials = Trials()\n",
    "        best = fmin(f, space4svm, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "\n",
    "        clf = SVC(kernel = 'linear', **best)\n",
    "        scaler.fit_transform(cv_set)\n",
    "        clf.fit(cv_set,y_cv)\n",
    "        # get distances of cv set\n",
    "        dist = clf.decision_function(cv_set)\n",
    "        # add distances to single results array\n",
    "        results_distances[indices_y] += dist\n",
    "        evalu = evalu.reshape(1, -1)\n",
    "        scaler.transform(evalu)\n",
    "        y_pred = clf.predict(evalu)\n",
    "        if y_pred == y_true:\n",
    "            result = 1\n",
    "        elif y_pred != y_true:\n",
    "            result = 0\n",
    "        evaluation_set.append(result)\n",
    "\n",
    "        eval_score = sum(evaluation_set) / len(evaluation_set)\n",
    "    evals_all.append(eval_score)\n",
    "    results_eval.loc['LOO Score (Eval)', 'All Metrics'] = np.median(evals_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ec637",
   "metadata": {},
   "source": [
    "# Hyperplane Distance\n",
    "\n",
    "* Calculate the distance between each participant and the hyperplane of the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean distances & save to dataframe\n",
    "# divide the distances by number of times they have been calculated to get mean\n",
    "# because of leave one out they have been calculated 34 times * 20\n",
    "final_dists = results_distances / ((len(y)-1)*20)\n",
    "\n",
    "results_dists = pd.DataFrame(columns=['Distance'])\n",
    "for sub_idx, sub in enumerate(df['Subject']):\n",
    "    results_dists.loc[sub, 'Distance'] = final_dists[sub_idx]\n",
    "\n",
    "\n",
    "# Classification Results:\n",
    "print(\"Classification Results\")\n",
    "display(results_eval)\n",
    "\n",
    "print(\"Distances\")\n",
    "display(results_dists)\n",
    "\n",
    "# save all accuracies df as csv\n",
    "# results_eval.to_csv('results_eval_acc.csv')\n",
    "# save distances\n",
    "# results_dists.to_csv('results_dists_34.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4077ea1",
   "metadata": {},
   "source": [
    "# Correlation between metrics and AQ/EQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48615f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_correlations = pd.DataFrame(columns=['ITV_ROI/AQ','ITV_ROI/EQ','ETV-slope/AQ','ETV-slope/EQ',\n",
    "                                             'ETV-variability/AQ','ETV-variability/EQ','ETV-variability_detrended/AQ',\n",
    "                                             'ETV-variability_detrended/EQ','ITV-ratio/AQ','ITV-ratio/EQ'])\n",
    "                                             \n",
    "# loop through every metric and quotient to correlate aq and eq with metric\n",
    "metric = ['ITV_ROI','ETV-slope','ETV-variability','ETV-variability_detrended','ITV-ratio']\n",
    "Q = ['AQ','EQ']\n",
    "for m in metric:\n",
    "    for q in Q:\n",
    "        column_name = f'{m}/{q}'\n",
    "        # correlate metric with AQ or EQ\n",
    "        r, p = pearsonr(df[m],df[q])\n",
    "        results_correlations.loc['r',column_name] = r\n",
    "        results_correlations.loc['p',column_name] = p\n",
    "\n",
    "display(results_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf4b90",
   "metadata": {},
   "source": [
    "# Correlation between Hyperplane distance and AQ/EQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae60c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty dataframe to put distance correlations in to\n",
    "results_dist_correlation = pd.DataFrame(columns=['Distance/AQ','Distance/EQ'])\n",
    "\n",
    "# loop through \n",
    "quotient = ['AQ', 'EQ']\n",
    "\n",
    "for q in quotient:\n",
    "    quot = df[q]\n",
    "    dist = results_dists['Distance']\n",
    "    r, p = pearsonr(dist,quot)\n",
    "    results_dist_correlation.loc['r',f'Distance/{q}'] = r\n",
    "    results_dist_correlation.loc['p',f'Distance/{q}'] = p\n",
    "display(results_dist_correlation)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b118330ceab869328094c9511ccecc9de71514160910ea41903be4aef43d8c30"
  },
  "kernelspec": {
   "display_name": "python 3.9.6 (instability)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
